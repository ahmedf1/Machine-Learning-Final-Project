{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression Using Gradient Descent And Stochastic Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the necessary modules (numpy, pandas, matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code cell using shift+enter before moving further\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets, linear_model, preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the column names in a list and use pandas read_csv to extract the dataframe from the file. Print the dataframe and the columns list to ensure that the mapping and the data was read correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names =['Algebra2/Trigonometry', \n",
    "        'English', 'Geometry', 'Global History and Geography','Integrated Algebra', \n",
    "        'Living Environment', 'Physical Settings/Chemistry', 'Physical Settings/Physics',\n",
    "        'Average SAT Score (Total)'\n",
    "]\n",
    "\n",
    "#Write your code below to save dataframe in the df variable below. \n",
    "# In place of None, write the pandas command to read the csv file.\n",
    "#df = pd.read_csv('HS_Regents_Sat_Scores_2015.csv',names=names)\n",
    "#print(df)\n",
    "#print(df.columns.tolist())\n",
    "#df = pd.read_csv('/Users/Farhad_Ahmed/Desktop/ML intros/ML Project/HS_Regents_Sat_Scores_2015.csv')\n",
    "df = pd.read_csv('HS_Regents_Sat_Scores_2015.csv')\n",
    "#print(df)\n",
    "#data = df.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we must define which features of the dataframe we will use. Then extract these values from the dataframe and do some feature scaling if necessary. Fetch those columns of data into a smaller dataframe. Then filter out the rows that do not contain valid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  52.6   71.9   66.4   61.    64.3   64.4   63.    64.7 1265. ]\n",
      " [  64.2   72.5   65.    66.9   63.5   70.8   65.7   71.5 1367. ]\n",
      " [  77.    74.    80.9   81.8   71.6   84.8   67.9   80.2 1700. ]\n",
      " [  81.2   88.2   84.2   90.9   88.2   90.7   83.1   79.9 1889. ]\n",
      " [  81.4   75.7   79.6   88.9   80.5   87.5   76.9   72.4 1704. ]\n",
      " [  64.    78.    72.1   72.6   72.6   73.2   73.1   74.1 1327. ]\n",
      " [  84.3   93.1   74.8   79.6   72.9   76.9   71.2   79.4 1511. ]\n",
      " [  92.9   91.5   93.1   94.3   93.7   93.6   88.7   91.5 2144. ]\n",
      " [  48.5   78.5   68.7   68.9   69.4   74.4   59.1   52.3 1358. ]\n",
      " [  51.3   74.6   64.3   60.2   68.3   64.3   54.2   50.4 1147. ]\n",
      " [  45.9   72.8   59.4   74.8   69.2   75.1   61.2   36.5 1292. ]\n",
      " [  67.8   83.2   75.5   81.    73.3   81.    66.2   63.2 1479. ]\n",
      " [  64.3   72.3   68.    65.    71.3   73.2   57.5   54.7 1116. ]\n",
      " [  81.9   78.2   82.5   85.1   84.7   83.    84.3   77.8 1529. ]\n",
      " [  56.    80.5   68.6   69.9   65.3   80.1   79.5   74.6 1257. ]\n",
      " [  54.9   74.3   70.5   68.1   64.7   75.8   63.1   71.7 1388. ]\n",
      " [  81.7   61.6   61.8   62.8   64.1   70.3   69.8   66.7 1226. ]\n",
      " [  61.5   79.    74.6   76.7   71.4   74.8   74.1   80.7 1480. ]\n",
      " [  73.4   84.6   80.9   87.4   76.9   89.5   72.1   70.2 1781. ]\n",
      " [  42.4   67.7   55.6   63.2   58.8   64.2   53.4   48.8 1234. ]\n",
      " [  81.4   85.6   78.5   87.7   78.7   86.6   75.3   66.  1647. ]\n",
      " [  76.9   83.1   77.5   80.    78.9   82.8   73.5   74.5 1556. ]\n",
      " [  58.2   76.5   69.2   77.6   76.4   80.5   68.9   68.7 1390. ]\n",
      " [  55.    67.2   56.7   55.    63.5   55.6   59.2   59.3 1135. ]\n",
      " [  39.4   68.1   51.8   62.2   61.8   64.9   54.2   40.9 1176. ]\n",
      " [  60.    71.8   61.8   64.8   64.    64.1   58.4   59.  1090. ]\n",
      " [  40.4   69.5   56.4   69.9   65.1   68.8   71.4   53.9 1194. ]\n",
      " [  62.3   64.6   53.4   56.4   63.    62.4   54.5   60.4 1252. ]\n",
      " [  58.8   67.4   50.5   59.1   63.    62.1   56.1   60.6 1188. ]\n",
      " [  53.9   58.7   60.9   54.4   58.    57.4   57.6   52.3 1182. ]\n",
      " [  37.8   67.2   48.3   56.6   66.9   59.5   57.8   62.9 1121. ]\n",
      " [  45.7   65.9   54.5   54.9   63.6   63.8   59.7   68.8 1268. ]\n",
      " [  52.2   72.1   55.3   72.5   69.5   71.7   54.2   69.1 1223. ]\n",
      " [  51.1   68.3   63.1   68.    65.6   73.8   57.    56.5 1214. ]\n",
      " [  43.8   65.5   59.1   54.7   64.4   61.2   60.3   65.2 1314. ]\n",
      " [  53.8   79.4   62.8   68.6   69.5   75.7   52.8   45.  1276. ]\n",
      " [  87.5   92.6   91.8   95.4   93.9   93.4   88.1   88.3 2041. ]\n",
      " [  60.8   73.    68.1   78.7   74.5   71.7   63.6   52.7 1291. ]\n",
      " [  85.4   93.4   86.9   93.2   85.5   94.8   85.4   90.  2013. ]\n",
      " [  47.1   74.6   54.9   64.5   61.3   65.7   55.7   57.8 1255. ]\n",
      " [  56.4   77.2   66.7   68.7   71.8   71.2   62.9   62.4 1407. ]\n",
      " [  55.6   69.9   63.    67.3   66.7   66.4   67.2   68.8 1248. ]\n",
      " [  47.6   63.6   61.8   62.2   60.6   68.7   48.1   61.5 1240. ]\n",
      " [  54.2   61.5   67.9   71.2   61.6   67.8   62.    49.7 1192. ]\n",
      " [  62.4   66.2   67.2   57.5   69.5   59.3   70.3   65.4 1128. ]\n",
      " [  57.2   76.6   66.3   68.2   72.3   67.3   53.1   45.9 1327. ]\n",
      " [  40.2   65.3   61.2   64.7   60.5   68.3   57.1   68.  1182. ]\n",
      " [  82.6   88.5   87.4   91.9   89.8   91.    78.4   80.7 1896. ]\n",
      " [  47.    67.6   55.6   58.1   61.    58.8   60.5   57.2 1216. ]\n",
      " [  58.5   69.8   61.3   75.6   62.7   74.7   69.2   62.2 1435. ]\n",
      " [  55.8   67.5   61.4   61.9   64.4   69.2   62.3   61.4 1334. ]\n",
      " [  33.6   71.7   59.7   64.4   63.9   70.6   54.6   35.9 1188. ]\n",
      " [  62.7   69.2   71.7   70.3   71.7   67.6   70.8   66.9 1313. ]\n",
      " [  67.    90.    74.6   83.2   74.7   83.9   71.1   66.8 1643. ]\n",
      " [  56.1   71.4   64.5   64.8   62.5   70.5   66.1   57.5 1179. ]\n",
      " [  57.5   84.8   69.4   77.7   67.4   76.8   71.    77.  1386. ]\n",
      " [  51.9   69.4   68.    68.9   64.1   69.    54.8   61.1 1099. ]\n",
      " [  69.7   66.5   74.1   69.8   68.9   74.6   73.3   83.  1327. ]\n",
      " [  57.1   73.2   58.5   67.7   68.    75.3   61.4   65.  1360. ]\n",
      " [  67.2   76.2   71.6   70.9   69.5   71.7   72.8   81.1 1420. ]\n",
      " [  73.5   71.    72.2   68.5   73.    77.    69.    73.2 1322. ]\n",
      " [  58.1   67.7   67.3   66.8   63.8   72.9   63.5   80.4 1287. ]\n",
      " [  72.7   50.4   63.2   62.8   62.2   62.2   59.2   57.3 1285. ]\n",
      " [  56.4   78.1   69.8   69.6   62.6   74.1   66.1   71.9 1451. ]\n",
      " [  52.7   62.3   63.7   69.3   68.2   70.1   60.4   58.2 1326. ]\n",
      " [  68.9   79.2   75.6   74.6   70.8   76.1   77.1   76.  1580. ]\n",
      " [  67.3   71.8   69.3   76.8   67.3   69.7   66.1   75.8 1386. ]\n",
      " [  74.    85.1   82.    81.1   64.7   82.8   73.9   76.8 1640. ]\n",
      " [  68.7   79.5   73.6   79.4   72.6   77.    75.9   84.9 1404. ]\n",
      " [  59.7   78.5   58.3   63.5   65.9   68.2   69.    76.3 1313. ]\n",
      " [  71.1   77.6   63.5   73.7   77.4   80.5   70.4   74.1 1277. ]\n",
      " [  49.5   65.7   65.4   64.9   64.5   63.3   65.5   76.6 1224. ]\n",
      " [  56.4   67.4   67.5   60.5   62.8   64.1   59.9   77.  1290. ]\n",
      " [  50.    66.9   60.5   72.7   72.7   72.3   63.6   66.8 1211. ]\n",
      " [  59.9   81.    71.2   81.5   66.    82.4   67.6   66.1 1427. ]\n",
      " [  47.6   70.4   59.7   76.2   70.9   66.1   54.1   54.4 1335. ]\n",
      " [  67.9   79.    77.6   77.8   75.    81.6   71.4   81.7 1431. ]\n",
      " [  55.4   74.1   66.1   64.    66.    69.9   68.6   75.5 1314. ]\n",
      " [  72.3   64.8   70.    57.    64.    64.    60.2   64.7 1256. ]\n",
      " [  90.7   91.9   93.7   95.2   87.2   92.7   87.9   87.1 1981. ]\n",
      " [  67.    60.    78.8   74.4   65.3   75.3   72.1   72.3 1578. ]\n",
      " [  77.8   76.8   82.5   76.8   72.9   80.8   77.6   79.5 1530. ]\n",
      " [  50.    68.2   58.5   58.9   63.7   65.1   71.9   71.2 1184. ]\n",
      " [  62.8   70.6   74.3   70.2   70.6   75.6   64.5   64.2 1487. ]\n",
      " [  61.7   74.6   64.5   69.3   62.4   72.9   67.6   64.2 1326. ]\n",
      " [  56.5   74.4   67.    69.4   70.8   71.3   64.8   68.1 1285. ]\n",
      " [  44.5   69.9   66.3   65.2   64.1   65.4   68.8   65.6 1218. ]\n",
      " [  59.7   67.7   64.2   63.5   66.4   62.2   66.7   61.4 1214. ]\n",
      " [  63.    79.6   72.5   77.2   76.1   79.1   68.6   63.8 1397. ]\n",
      " [  69.1   80.3   75.5   79.1   75.1   78.9   71.4   78.2 1193. ]\n",
      " [  63.5   77.    67.9   68.8   69.8   70.4   69.4   64.4 1410. ]\n",
      " [  73.4   79.3   76.2   75.8   69.1   77.2   70.3   71.6 1485. ]\n",
      " [  58.8   70.3   62.9   67.5   66.4   66.8   60.8   56.7 1306. ]\n",
      " [  77.7   79.4   75.6   73.4   70.    80.6   70.7   75.3 1457. ]\n",
      " [  48.    63.8   56.8   60.1   57.9   62.2   55.    58.7 1248. ]\n",
      " [  43.7   69.5   49.8   58.4   62.7   64.3   55.1   47.2 1253. ]\n",
      " [  52.3   81.9   69.7   77.    71.3   79.1   58.4   41.4 1346. ]\n",
      " [  62.4   66.6   65.2   63.5   69.7   70.    69.4   67.1 1316. ]\n",
      " [  41.2   67.4   54.6   61.9   65.7   63.9   51.9   55.6 1265. ]\n",
      " [  64.5   87.4   75.2   81.6   61.7   84.2   67.3   74.3 1622. ]\n",
      " [  75.1   71.8   77.2   67.2   64.5   75.1   76.3   72.1 1274. ]\n",
      " [  67.    65.6   75.    63.4   72.6   77.1   69.3   79.8 1245. ]\n",
      " [  53.8   85.1   74.    79.5   72.5   81.5   77.2   71.9 1409. ]\n",
      " [  58.1   74.4   66.3   66.9   67.2   72.7   66.7   70.  1344. ]\n",
      " [  57.9   67.9   60.3   59.7   65.4   67.    62.8   67.6 1284. ]\n",
      " [  54.5   71.1   62.    63.1   66.5   67.6   64.4   64.  1355. ]\n",
      " [  61.5   78.7   66.7   71.6   66.6   73.4   63.7   66.2 1446. ]\n",
      " [  62.3   77.7   66.5   71.8   65.4   73.3   68.9   71.7 1473. ]\n",
      " [  52.8   69.8   66.7   60.7   63.5   64.5   61.7   61.1 1141. ]]\n",
      "(109, 9)\n"
     ]
    }
   ],
   "source": [
    "#  Also, save the values in df2 after dropping empty rows from df1\n",
    "# DEFINE Columns here -->\n",
    "# extract all the columns that we need\n",
    "df1=np.stack((df['Algebra2/Trigonometry'], df['English'], df['Geometry'],df['Global History and Geography'], df['Integrated Algebra'], df['Living Environment'], df['Physical Settings/Chemistry'], df['Physical Settings/Physics'], df['Average SAT Score (Total)'])).T\n",
    "\n",
    "#remove any empty rows\n",
    "df2=(df1[~np.isnan(df1).any(axis=1)])\n",
    "print(df2)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vector and fill with the output values (SAT Score). Create a matrix and fill with the features extracted from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109, 8)\n",
      "(109, 1)\n"
     ]
    }
   ],
   "source": [
    "# df3 will be our feature matrix\n",
    "df3 = df2[:,:8]\n",
    "#print(df3)\n",
    "\n",
    "# df4 will be the target vector\n",
    "df4 = df2[:,8:]\n",
    "#print(df4)\n",
    "X2 = np.array(df3)\n",
    "Y2= np.array(df4)\n",
    "\n",
    "X = np.array(df3)\n",
    "Y = np.array(df4)\n",
    "\n",
    "# Check the shape of x and y vectors.\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the assigment does some reshaping here to convert the rank of the y vector. But the rank of the observation matrix can tell you if it is possible to find a unique solution for your model, which almost never happens. https://stats.stackexchange.com/questions/152369/does-rank-of-observation-matrix-tell-anything-useful-when-applying-machine-learn\n",
    "\n",
    "So don't know if this part is completely necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109, 1)\n"
     ]
    }
   ],
   "source": [
    "Y=Y.reshape(Y.shape[0],1) \n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the value of n i.e. number of training examples. Hint: Value of n is equal to the number of rows in either x or y matrix which can be accessed using numpy shape command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    }
   ],
   "source": [
    "n=X.shape[0]\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending a Column of ones in x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.ones((Y.shape[0],1))\n",
    "X=np.hstack((a , X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109, 9)\n"
     ]
    }
   ],
   "source": [
    "# Shape of x should be [number of training examples (n)] x [number of features + 1]\n",
    "# the 1 comes from the column of ones we just added\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, w, n):\n",
    "    #Cost can be calculated using a single line of code.\n",
    "    # Remember w is a vector here.\n",
    "    cost=(1/(2*n)) * np.dot(np.ones(n).T, (np.dot(x,w)-y)**2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x , y , learning_rate , w , n , num_iters):\n",
    "    # write the updated value of w in temp \n",
    "    for i in range(num_iters):\n",
    "        # derivative vector is given by : X_train.Transpose *  (( X_train * w_vector)- y ) \n",
    "        temp =  w - (learning_rate/n)*np.dot(x.T,(np.dot(x,w)-y))\n",
    "        w = temp\n",
    "        cost= compute_cost(x , y , w , n)\n",
    "        print(\"[Cost:  ]\")\n",
    "        print(cost)\n",
    "        print(w)\n",
    "        \n",
    "            \n",
    "            \n",
    "    return w  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating the Batch Gradient Descent Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrating the above function into a single function multiple_linear_reg_model_gda: This function uses gradient descent algorithm to minimize the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_linear_reg_model_gda(x , y , n , learning_rate , num_iters):\n",
    "    #initialize the values of parameter vector w. It should be a column vector of zeros of dimension(n,1)\n",
    "    w = np.zeros((x.shape[1],1))\n",
    "\n",
    "    #calculate the initial cost by calling the function you coded above.\n",
    "    initial_cost= compute_cost(x, y , w, n)\n",
    "    print(\"Initial Cost\")\n",
    "    print(initial_cost)\n",
    "    \n",
    "    #calculate the optimized value of gradients by calling the gradient_descent function coded above\n",
    "    w = gradient_descent(x , y , learning_rate , w , n , num_iters)\n",
    "    \n",
    "    #Calculate the cost with the optimized value of w by calling the cost function.\n",
    "    \n",
    "    final_cost = compute_cost(x, y , w, n)\n",
    "    print(\"Final Cost\")\n",
    "    print(final_cost)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when you have coded multiple_linear_reg_model_gda function, you can call this function to find the optimized values of parameters w. Before calling the function, set the values of learning_rate and num_iters. You may have to call this function several number of times with different values of num_iters and learning_rate to find the optimal values of w. For some values of learning_rate, it may give an error as the values of cost may reach a very high value(infinity) due to overshooting as discussed in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.01291137]\n",
      " [ 1.81567226]\n",
      " [ 2.41995147]\n",
      " [ 8.45018001]\n",
      " [ 0.86058026]\n",
      " [ 2.50727586]\n",
      " [-0.81495501]\n",
      " [ 1.5790801 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 0.0000029291\n",
    "num_iters = 10000000\n",
    "\n",
    "n = Y.shape[0]\n",
    "w = multiple_linear_reg_model_gda(X , Y , n , learning_rate , num_iters)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equation Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will be writing the code to find the values of parameters w for our multiple linear regression model. This can also be used to cross-check the optimal values of w we just found above using method above. These values should be same (or nearly same). Instead of writing the code for normal equation in one line, you can break this into 3 parts: First calculate q=inverse of (dot of (X.T,X)) (these are pseudo commands, use original numpy commands to calculate q). Then w= dot of ( X.T , y) and then beta_vec= dot of (q,w). Here, w_vec is vector of dimension (5,1) having two values. Example w0=w_vec[0][0].\n",
    "Note : Do not append a column of ones in x because you have already done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.01291137]\n",
      " [ 1.81567226]\n",
      " [ 2.41995147]\n",
      " [ 8.45018001]\n",
      " [ 0.86058026]\n",
      " [ 2.50727586]\n",
      " [-0.81495501]\n",
      " [ 1.5790801 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "q = np.linalg.inv(np.dot(X2.T,X2))\n",
    "w = np.dot(X2.T,Y2)\n",
    "w_vec = np.dot(q,w)\n",
    "\n",
    "#RSS\n",
    "#print(np.sum((np.dot(x,w_vec)-y)**2))\n",
    "#n=506\n",
    "#TSS\n",
    "#print(np.sum((y-(np.sum(y)/n))**2))\n",
    "\n",
    "print(w_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Values of beta you just got above should be approximately same as the ones you got using multiple_linear_reg_model_gda. This assignment ends here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
