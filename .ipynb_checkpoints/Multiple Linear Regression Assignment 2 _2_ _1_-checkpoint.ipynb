{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Multiple Linear Regression Using Gradient Descent And Stochastic Gradient Descent Algorithm.\n",
    "\n",
    "In this assignment, you will be implementing Multiple Linear Regression Model on the same data set as in programming assignment 1, but here we will be using gradient descent algorithm (GDA) and stochastic gradient descent algorithm (SGDA) to minimize the cost function as taught in the class. The SGDA implementation is optional.\n",
    "\n",
    "Please add your own print statements to check your code to ensure your code is correct in every step.  (Note: we will not be grading the print statements you add to your code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code cell using shift+enter before moving further\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data set.\n",
    "\n",
    "In the below code cell, you will load the data using python pandas library as done in the programming_assignment_1b. Use pd.read_csv('File url ', header=None,.... ) with the value of header=None,delim_whitespace=True,names=names,na_values='?' as attributes. The url for the .data file is https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data. This step is same as done in programming_assignment_1b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n",
      "0     0.00632  18.0   2.31     0  0.538  6.575   65.2  4.0900    1  296.0   \n",
      "1     0.02731   0.0   7.07     0  0.469  6.421   78.9  4.9671    2  242.0   \n",
      "2     0.02729   0.0   7.07     0  0.469  7.185   61.1  4.9671    2  242.0   \n",
      "3     0.03237   0.0   2.18     0  0.458  6.998   45.8  6.0622    3  222.0   \n",
      "4     0.06905   0.0   2.18     0  0.458  7.147   54.2  6.0622    3  222.0   \n",
      "5     0.02985   0.0   2.18     0  0.458  6.430   58.7  6.0622    3  222.0   \n",
      "6     0.08829  12.5   7.87     0  0.524  6.012   66.6  5.5605    5  311.0   \n",
      "7     0.14455  12.5   7.87     0  0.524  6.172   96.1  5.9505    5  311.0   \n",
      "8     0.21124  12.5   7.87     0  0.524  5.631  100.0  6.0821    5  311.0   \n",
      "9     0.17004  12.5   7.87     0  0.524  6.004   85.9  6.5921    5  311.0   \n",
      "10    0.22489  12.5   7.87     0  0.524  6.377   94.3  6.3467    5  311.0   \n",
      "11    0.11747  12.5   7.87     0  0.524  6.009   82.9  6.2267    5  311.0   \n",
      "12    0.09378  12.5   7.87     0  0.524  5.889   39.0  5.4509    5  311.0   \n",
      "13    0.62976   0.0   8.14     0  0.538  5.949   61.8  4.7075    4  307.0   \n",
      "14    0.63796   0.0   8.14     0  0.538  6.096   84.5  4.4619    4  307.0   \n",
      "15    0.62739   0.0   8.14     0  0.538  5.834   56.5  4.4986    4  307.0   \n",
      "16    1.05393   0.0   8.14     0  0.538  5.935   29.3  4.4986    4  307.0   \n",
      "17    0.78420   0.0   8.14     0  0.538  5.990   81.7  4.2579    4  307.0   \n",
      "18    0.80271   0.0   8.14     0  0.538  5.456   36.6  3.7965    4  307.0   \n",
      "19    0.72580   0.0   8.14     0  0.538  5.727   69.5  3.7965    4  307.0   \n",
      "20    1.25179   0.0   8.14     0  0.538  5.570   98.1  3.7979    4  307.0   \n",
      "21    0.85204   0.0   8.14     0  0.538  5.965   89.2  4.0123    4  307.0   \n",
      "22    1.23247   0.0   8.14     0  0.538  6.142   91.7  3.9769    4  307.0   \n",
      "23    0.98843   0.0   8.14     0  0.538  5.813  100.0  4.0952    4  307.0   \n",
      "24    0.75026   0.0   8.14     0  0.538  5.924   94.1  4.3996    4  307.0   \n",
      "25    0.84054   0.0   8.14     0  0.538  5.599   85.7  4.4546    4  307.0   \n",
      "26    0.67191   0.0   8.14     0  0.538  5.813   90.3  4.6820    4  307.0   \n",
      "27    0.95577   0.0   8.14     0  0.538  6.047   88.8  4.4534    4  307.0   \n",
      "28    0.77299   0.0   8.14     0  0.538  6.495   94.4  4.4547    4  307.0   \n",
      "29    1.00245   0.0   8.14     0  0.538  6.674   87.3  4.2390    4  307.0   \n",
      "..        ...   ...    ...   ...    ...    ...    ...     ...  ...    ...   \n",
      "476   4.87141   0.0  18.10     0  0.614  6.484   93.6  2.3053   24  666.0   \n",
      "477  15.02340   0.0  18.10     0  0.614  5.304   97.3  2.1007   24  666.0   \n",
      "478  10.23300   0.0  18.10     0  0.614  6.185   96.7  2.1705   24  666.0   \n",
      "479  14.33370   0.0  18.10     0  0.614  6.229   88.0  1.9512   24  666.0   \n",
      "480   5.82401   0.0  18.10     0  0.532  6.242   64.7  3.4242   24  666.0   \n",
      "481   5.70818   0.0  18.10     0  0.532  6.750   74.9  3.3317   24  666.0   \n",
      "482   5.73116   0.0  18.10     0  0.532  7.061   77.0  3.4106   24  666.0   \n",
      "483   2.81838   0.0  18.10     0  0.532  5.762   40.3  4.0983   24  666.0   \n",
      "484   2.37857   0.0  18.10     0  0.583  5.871   41.9  3.7240   24  666.0   \n",
      "485   3.67367   0.0  18.10     0  0.583  6.312   51.9  3.9917   24  666.0   \n",
      "486   5.69175   0.0  18.10     0  0.583  6.114   79.8  3.5459   24  666.0   \n",
      "487   4.83567   0.0  18.10     0  0.583  5.905   53.2  3.1523   24  666.0   \n",
      "488   0.15086   0.0  27.74     0  0.609  5.454   92.7  1.8209    4  711.0   \n",
      "489   0.18337   0.0  27.74     0  0.609  5.414   98.3  1.7554    4  711.0   \n",
      "490   0.20746   0.0  27.74     0  0.609  5.093   98.0  1.8226    4  711.0   \n",
      "491   0.10574   0.0  27.74     0  0.609  5.983   98.8  1.8681    4  711.0   \n",
      "492   0.11132   0.0  27.74     0  0.609  5.983   83.5  2.1099    4  711.0   \n",
      "493   0.17331   0.0   9.69     0  0.585  5.707   54.0  2.3817    6  391.0   \n",
      "494   0.27957   0.0   9.69     0  0.585  5.926   42.6  2.3817    6  391.0   \n",
      "495   0.17899   0.0   9.69     0  0.585  5.670   28.8  2.7986    6  391.0   \n",
      "496   0.28960   0.0   9.69     0  0.585  5.390   72.9  2.7986    6  391.0   \n",
      "497   0.26838   0.0   9.69     0  0.585  5.794   70.6  2.8927    6  391.0   \n",
      "498   0.23912   0.0   9.69     0  0.585  6.019   65.3  2.4091    6  391.0   \n",
      "499   0.17783   0.0   9.69     0  0.585  5.569   73.5  2.3999    6  391.0   \n",
      "500   0.22438   0.0   9.69     0  0.585  6.027   79.7  2.4982    6  391.0   \n",
      "501   0.06263   0.0  11.93     0  0.573  6.593   69.1  2.4786    1  273.0   \n",
      "502   0.04527   0.0  11.93     0  0.573  6.120   76.7  2.2875    1  273.0   \n",
      "503   0.06076   0.0  11.93     0  0.573  6.976   91.0  2.1675    1  273.0   \n",
      "504   0.10959   0.0  11.93     0  0.573  6.794   89.3  2.3889    1  273.0   \n",
      "505   0.04741   0.0  11.93     0  0.573  6.030   80.8  2.5050    1  273.0   \n",
      "\n",
      "     PTRATIO       B  LSTAT  PRICE  \n",
      "0       15.3  396.90   4.98   24.0  \n",
      "1       17.8  396.90   9.14   21.6  \n",
      "2       17.8  392.83   4.03   34.7  \n",
      "3       18.7  394.63   2.94   33.4  \n",
      "4       18.7  396.90   5.33   36.2  \n",
      "5       18.7  394.12   5.21   28.7  \n",
      "6       15.2  395.60  12.43   22.9  \n",
      "7       15.2  396.90  19.15   27.1  \n",
      "8       15.2  386.63  29.93   16.5  \n",
      "9       15.2  386.71  17.10   18.9  \n",
      "10      15.2  392.52  20.45   15.0  \n",
      "11      15.2  396.90  13.27   18.9  \n",
      "12      15.2  390.50  15.71   21.7  \n",
      "13      21.0  396.90   8.26   20.4  \n",
      "14      21.0  380.02  10.26   18.2  \n",
      "15      21.0  395.62   8.47   19.9  \n",
      "16      21.0  386.85   6.58   23.1  \n",
      "17      21.0  386.75  14.67   17.5  \n",
      "18      21.0  288.99  11.69   20.2  \n",
      "19      21.0  390.95  11.28   18.2  \n",
      "20      21.0  376.57  21.02   13.6  \n",
      "21      21.0  392.53  13.83   19.6  \n",
      "22      21.0  396.90  18.72   15.2  \n",
      "23      21.0  394.54  19.88   14.5  \n",
      "24      21.0  394.33  16.30   15.6  \n",
      "25      21.0  303.42  16.51   13.9  \n",
      "26      21.0  376.88  14.81   16.6  \n",
      "27      21.0  306.38  17.28   14.8  \n",
      "28      21.0  387.94  12.80   18.4  \n",
      "29      21.0  380.23  11.98   21.0  \n",
      "..       ...     ...    ...    ...  \n",
      "476     20.2  396.21  18.68   16.7  \n",
      "477     20.2  349.48  24.91   12.0  \n",
      "478     20.2  379.70  18.03   14.6  \n",
      "479     20.2  383.32  13.11   21.4  \n",
      "480     20.2  396.90  10.74   23.0  \n",
      "481     20.2  393.07   7.74   23.7  \n",
      "482     20.2  395.28   7.01   25.0  \n",
      "483     20.2  392.92  10.42   21.8  \n",
      "484     20.2  370.73  13.34   20.6  \n",
      "485     20.2  388.62  10.58   21.2  \n",
      "486     20.2  392.68  14.98   19.1  \n",
      "487     20.2  388.22  11.45   20.6  \n",
      "488     20.1  395.09  18.06   15.2  \n",
      "489     20.1  344.05  23.97    7.0  \n",
      "490     20.1  318.43  29.68    8.1  \n",
      "491     20.1  390.11  18.07   13.6  \n",
      "492     20.1  396.90  13.35   20.1  \n",
      "493     19.2  396.90  12.01   21.8  \n",
      "494     19.2  396.90  13.59   24.5  \n",
      "495     19.2  393.29  17.60   23.1  \n",
      "496     19.2  396.90  21.14   19.7  \n",
      "497     19.2  396.90  14.10   18.3  \n",
      "498     19.2  396.90  12.92   21.2  \n",
      "499     19.2  395.77  15.10   17.5  \n",
      "500     19.2  396.90  14.33   16.8  \n",
      "501     21.0  391.99   9.67   22.4  \n",
      "502     21.0  396.90   9.08   20.6  \n",
      "503     21.0  396.90   5.64   23.9  \n",
      "504     21.0  393.45   6.48   22.0  \n",
      "505     21.0  396.90   7.88   11.9  \n",
      "\n",
      "[506 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# After completing the code in this code cell, run this code cell before moving further.\n",
    "names =[\n",
    "    'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', \n",
    "    'AGE',  'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'PRICE'\n",
    "]\n",
    "\n",
    "#Write your code below to save dataframe in the df variable below. \n",
    "# In place of None, write the pandas command to read the csv file.\n",
    "df= pd.read_csv('data.txt', header=None,delim_whitespace=True,names=names,na_values='?')\n",
    "#print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of taking just 1 feature i.e 'RM' as we did in Simple Linear Regression, we will use 4 features which are 'LSTAT', 'DIS', 'RM' and 'INDUS'. We choose only these features because they are almost on similar scale. So, there is no need for feature scaling. Fetching the values of 5 columns into a smaller dataframe (say) df1 from df. This step is also same as done in programming assignment 1.There we fetched only 2 columns but here we need 5 columns.We need the values in the 'PRICE', 'LSTAT', 'DIS', 'RM' and 'INDUS' columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  After completing the code in this code cell, run this code cell before moving further. Also, save the values in df2 after dropping empty rows from df1\n",
    "# Write your code below.\n",
    "df1=None\n",
    "df2=None\n",
    "# Check the shape of df2. It should be (506,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vector y having the values of 'PRICE' column and matrix x having the values of  'LSTAT', 'DIS', 'RM' and 'INDUS' columns. We have already written the code for that. Just run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just run this code cell\n",
    "df3 = df2[['LSTAT', 'DIS', 'RM', 'INDUS']]\n",
    "x=df3.values\n",
    "y=np.array(df2['PRICE'])\n",
    "# Check the shape of x and y vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape y to be rank 2. After checking the shape of x and y in the above code cell, we see that x is already rank 2 and y are rank 1 matrix. Before moving ahead, convert y to be rank 2 matrix. For example, I would use the command y=y.reshape(y.shape[0],1) to reshape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After completing the code in this code cell, run this code cell before moving further. \n",
    "# Write your code below\n",
    "y=None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the value of n i.e. number of training examples. \n",
    "Hint: Value of n is equal to the number of rows in either x or y matrix which can be accessed using numpy shape command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After completing the code in this code cell, run this code cell before moving further. \n",
    "# Write your code below\n",
    "n=None\n",
    "# After writing a code, it is a good practice to verify that your code is correct. \n",
    "# For example, in this case you can print the value of n and check that it should be equal to 506.\n",
    "# Occasionally we created test code cell to verify your code. \n",
    "# However, you should do it for almost every code you write."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appending a Column of ones in x: Just run the code cell below\n",
    "Warning: Just run the below code cell only once or you would be appending column of ones multiple times if you run the cell multiple times. If by mistake you run the code cell more than once, re-initilize the value of x again in the previous code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=np.ones((y.shape[0],1))\n",
    "x=np.hstack((a , x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shape of x should be (506,5)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function.\n",
    "Compute the cost: Write the code to compute the cost inside the function. Do not change the function name or function parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(x, y, w, n):\n",
    "    #Write your code in place of None. Cost can be calculated using a single line of code.\n",
    "    # Remember w is a vector here.\n",
    "    cost=None\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving ahead, ensure that the code you have written to compute the cost is correct. Just run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_testcase=np.zeros((5,1))\n",
    "cost_verify= compute_cost(x, y, w_testcase, n)\n",
    "print(cost_verify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your output should be equal to 296.073458498. If it's equal to this, then move ahead. Else, re-check your code and re-verify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "Write the code to perform gradient descent in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x , y , learning_rate , w , n , num_iters):\n",
    "    # In place of None, write the updated value of w in temp \n",
    "    for i in range(num_iters):\n",
    "        # derivative vector is given by : X_train.Transpose *  (( X_train * w_vector)- y ) \n",
    "        temp =  None\n",
    "        w = None\n",
    "       \n",
    "        \n",
    "        if(i%100==0):\n",
    "            # In place of None, call the cost you just coded above\n",
    "            cost= None\n",
    "            print(\"Cost\")\n",
    "            print(cost)\n",
    "             \n",
    "            \n",
    "            \n",
    "    return w      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving ahead, ensure that your code to update w is correct. Run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_testcase=np.zeros((5,1))\n",
    "g=gradient_descent(x , y , 0.0049 , w_testcase , n , 100000)\n",
    "print(g[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last output should be: 7.46236374. If it matches your out, then your code is likely to be correct. Otherwise, re-check your code and re-verify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating the Batch Gradient Descent Function \n",
    "\n",
    "Integrating the above function into a single function multiple_linear_reg_model_gda: This function uses gradient descent algorithm to minimize the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiple_linear_reg_model_gda(x , y , n , learning_rate , num_iters):\n",
    "    #initialize the values of parameter vector w. It should be a column vector of zeros of dimension(n,1)\n",
    "    w = None\n",
    "    \n",
    "    #calculate the initial cost by calling the function you coded above.\n",
    "    initial_cost= None\n",
    "    print(\"Initial Cost\")\n",
    "    print(initial_cost)\n",
    "    \n",
    "    #calculate the optimized value of gradients by calling the gradient_descent function coded above\n",
    "    \n",
    "    w = None\n",
    "    \n",
    "    #Calculate the cost with the optimized value of w by calling the cost function.\n",
    "    \n",
    "    final_cost = None\n",
    "    print(\"Final Cost\")\n",
    "    print(final_cost)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when you have coded multiple_linear_reg_model_gda function, you can call this function to find the optimized values of parameters w. Before calling the function, set the values of learning_rate and num_iters. You may have to call this function several number of times with different values of num_iters and learning_rate to find the optimal values of w. For some values of learning_rate, it may give an error as the values of cost may reach a very high value(infinity) due to overshooting as discussed in the class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code below\n",
    "learning_rate = None\n",
    "num_iters = None\n",
    "# In place of None, call the multiple_linear_reg_model_gda.\n",
    "w = None\n",
    "\n",
    "# The value of final cost should be 14.3470049896 or nearly this(depending on the values of learning_rate and num_itersations you choose.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Equation Method\n",
    "Now, we will be writing the code to find the values of parameters w for our multiple linear regression model. This can also be used to cross-check the optimal values of w we just found above using method above. These values should be same (or nearly same).\n",
    "Instead of writing the code for normal equation in one line, you can break this into 3 parts: First calculate q=inverse of (dot of (X.T,X)) (these are pseudo commands, use original numpy commands to calculate q). Then w= dot of ( X.T , y) and then beta_vec= dot of (q,w). Here, w_vec is vector of dimension (5,1) having two values. Example w0=w_vec[0][0].\n",
    "\n",
    "Note : Do not append a column of ones in x because you have already done before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the code below\n",
    "q = None\n",
    "w = None\n",
    "w_vec = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Values of beta you just got above should be approximately same as the ones you got using multiple_linear_reg_model_gda.\n",
    "This assignment ends here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Stochastic Gradient Descent (Optional / Ungraded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(x , y , learning_rate , w , n , num_iters):\n",
    "\n",
    "    for j in range(num_iters):\n",
    "    \n",
    "        for i in range(0,n):\n",
    "        # Write updated value of w vector in place of None\n",
    "            temp = None \n",
    "        \n",
    "            w = temp\n",
    "   \n",
    "\n",
    "        if(j%2000==0):\n",
    "            cost= None\n",
    "            print(\"Cost\")\n",
    "            print(cost)\n",
    "            \n",
    "            \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiple_linear_reg_model_sgda(x , y , n , learning_rate , num_iters):\n",
    "    #initialize the values of parameter vector w. It should be a column vector of zeros of dimension(m,1)\n",
    "    w = None\n",
    "    \n",
    "    #calculate the initial cost by calling the function you coded above.\n",
    "    initial_cost=None\n",
    "    print(\"Initial Cost\")\n",
    "    print(initial_cost)\n",
    "    \n",
    "    #calculate the optimized value of gradients by calling the stochastic_gradient_descent function coded above\n",
    "    \n",
    "    w = None\n",
    "    \n",
    "    #Calculate the cost with the optimized value of w by calling the cost function.\n",
    "    \n",
    "    final_cost=None\n",
    "    print(\"Final Cost\")\n",
    "    print(final_cost)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when you have coded multiple_linear_reg_model_sgda function, you can call this function to find the optimized values of parameters w. Before calling the function, set the values of learning_rate and num_iters. You may have to call this function several number of times with different values of num_iters and learning_rate to find the optimal values of parameters w. For some values of learning_rate, it may give an error as the values of parameters may reach a very high value(infinity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code below\n",
    "learning_rate=None\n",
    "num_iters=None\n",
    "# In place of None, call the function multiple_linear_reg_model_sgda.\n",
    "w=None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
