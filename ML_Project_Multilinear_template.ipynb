{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression Using Gradient Descent And Stochastic Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the necessary modules (numpy, pandas, matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code cell using shift+enter before moving further\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets, linear_model, preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the column names in a list and use pandas read_csv to extract the dataframe from the file. Print the dataframe and the columns list to ensure that the mapping and the data was read correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names =['Algebra2/Trigonometry', \n",
    "        'English', 'Geometry', 'Global History and Geography','Integrated Algebra', \n",
    "        'Living Environment', 'Physical Settings/Chemistry', 'Physical Settings/Physics',\n",
    "        'Average SAT Score (Total)'\n",
    "]\n",
    "\n",
    "#Write your code below to save dataframe in the df variable below. \n",
    "# In place of None, write the pandas command to read the csv file.\n",
    "#df = pd.read_csv('HS_Regents_Sat_Scores_2015.csv',names=names)\n",
    "#print(df)\n",
    "#print(df.columns.tolist())\n",
    "#df = pd.read_csv('/Users/Farhad_Ahmed/Desktop/ML intros/ML Project/HS_Regents_Sat_Scores_2015.csv')\n",
    "df = pd.read_csv('HS_Regents_Sat_Scores_2015.csv')\n",
    "#print(df)\n",
    "#data = df.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we must define which features of the dataframe we will use. Then extract these values from the dataframe and do some feature scaling if necessary. Fetch those columns of data into a smaller dataframe. Then filter out the rows that do not contain valid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  52.6   71.9   66.4   61.    64.3   64.4   63.    64.7 1265. ]\n",
      " [  64.2   72.5   65.    66.9   63.5   70.8   65.7   71.5 1367. ]\n",
      " [  77.    74.    80.9   81.8   71.6   84.8   67.9   80.2 1700. ]\n",
      " [  81.2   88.2   84.2   90.9   88.2   90.7   83.1   79.9 1889. ]\n",
      " [  81.4   75.7   79.6   88.9   80.5   87.5   76.9   72.4 1704. ]\n",
      " [  64.    78.    72.1   72.6   72.6   73.2   73.1   74.1 1327. ]\n",
      " [  84.3   93.1   74.8   79.6   72.9   76.9   71.2   79.4 1511. ]\n",
      " [  92.9   91.5   93.1   94.3   93.7   93.6   88.7   91.5 2144. ]\n",
      " [  48.5   78.5   68.7   68.9   69.4   74.4   59.1   52.3 1358. ]\n",
      " [  51.3   74.6   64.3   60.2   68.3   64.3   54.2   50.4 1147. ]\n",
      " [  45.9   72.8   59.4   74.8   69.2   75.1   61.2   36.5 1292. ]\n",
      " [  67.8   83.2   75.5   81.    73.3   81.    66.2   63.2 1479. ]\n",
      " [  64.3   72.3   68.    65.    71.3   73.2   57.5   54.7 1116. ]\n",
      " [  81.9   78.2   82.5   85.1   84.7   83.    84.3   77.8 1529. ]\n",
      " [  56.    80.5   68.6   69.9   65.3   80.1   79.5   74.6 1257. ]\n",
      " [  54.9   74.3   70.5   68.1   64.7   75.8   63.1   71.7 1388. ]\n",
      " [  81.7   61.6   61.8   62.8   64.1   70.3   69.8   66.7 1226. ]\n",
      " [  61.5   79.    74.6   76.7   71.4   74.8   74.1   80.7 1480. ]\n",
      " [  73.4   84.6   80.9   87.4   76.9   89.5   72.1   70.2 1781. ]\n",
      " [  42.4   67.7   55.6   63.2   58.8   64.2   53.4   48.8 1234. ]\n",
      " [  81.4   85.6   78.5   87.7   78.7   86.6   75.3   66.  1647. ]\n",
      " [  76.9   83.1   77.5   80.    78.9   82.8   73.5   74.5 1556. ]\n",
      " [  58.2   76.5   69.2   77.6   76.4   80.5   68.9   68.7 1390. ]\n",
      " [  55.    67.2   56.7   55.    63.5   55.6   59.2   59.3 1135. ]\n",
      " [  39.4   68.1   51.8   62.2   61.8   64.9   54.2   40.9 1176. ]\n",
      " [  60.    71.8   61.8   64.8   64.    64.1   58.4   59.  1090. ]\n",
      " [  40.4   69.5   56.4   69.9   65.1   68.8   71.4   53.9 1194. ]\n",
      " [  62.3   64.6   53.4   56.4   63.    62.4   54.5   60.4 1252. ]\n",
      " [  58.8   67.4   50.5   59.1   63.    62.1   56.1   60.6 1188. ]\n",
      " [  53.9   58.7   60.9   54.4   58.    57.4   57.6   52.3 1182. ]\n",
      " [  37.8   67.2   48.3   56.6   66.9   59.5   57.8   62.9 1121. ]\n",
      " [  45.7   65.9   54.5   54.9   63.6   63.8   59.7   68.8 1268. ]\n",
      " [  52.2   72.1   55.3   72.5   69.5   71.7   54.2   69.1 1223. ]\n",
      " [  51.1   68.3   63.1   68.    65.6   73.8   57.    56.5 1214. ]\n",
      " [  43.8   65.5   59.1   54.7   64.4   61.2   60.3   65.2 1314. ]\n",
      " [  53.8   79.4   62.8   68.6   69.5   75.7   52.8   45.  1276. ]\n",
      " [  87.5   92.6   91.8   95.4   93.9   93.4   88.1   88.3 2041. ]\n",
      " [  60.8   73.    68.1   78.7   74.5   71.7   63.6   52.7 1291. ]\n",
      " [  85.4   93.4   86.9   93.2   85.5   94.8   85.4   90.  2013. ]\n",
      " [  47.1   74.6   54.9   64.5   61.3   65.7   55.7   57.8 1255. ]\n",
      " [  56.4   77.2   66.7   68.7   71.8   71.2   62.9   62.4 1407. ]\n",
      " [  55.6   69.9   63.    67.3   66.7   66.4   67.2   68.8 1248. ]\n",
      " [  47.6   63.6   61.8   62.2   60.6   68.7   48.1   61.5 1240. ]\n",
      " [  54.2   61.5   67.9   71.2   61.6   67.8   62.    49.7 1192. ]\n",
      " [  62.4   66.2   67.2   57.5   69.5   59.3   70.3   65.4 1128. ]\n",
      " [  57.2   76.6   66.3   68.2   72.3   67.3   53.1   45.9 1327. ]\n",
      " [  40.2   65.3   61.2   64.7   60.5   68.3   57.1   68.  1182. ]\n",
      " [  82.6   88.5   87.4   91.9   89.8   91.    78.4   80.7 1896. ]\n",
      " [  47.    67.6   55.6   58.1   61.    58.8   60.5   57.2 1216. ]\n",
      " [  58.5   69.8   61.3   75.6   62.7   74.7   69.2   62.2 1435. ]\n",
      " [  55.8   67.5   61.4   61.9   64.4   69.2   62.3   61.4 1334. ]\n",
      " [  33.6   71.7   59.7   64.4   63.9   70.6   54.6   35.9 1188. ]\n",
      " [  62.7   69.2   71.7   70.3   71.7   67.6   70.8   66.9 1313. ]\n",
      " [  67.    90.    74.6   83.2   74.7   83.9   71.1   66.8 1643. ]\n",
      " [  56.1   71.4   64.5   64.8   62.5   70.5   66.1   57.5 1179. ]\n",
      " [  57.5   84.8   69.4   77.7   67.4   76.8   71.    77.  1386. ]\n",
      " [  51.9   69.4   68.    68.9   64.1   69.    54.8   61.1 1099. ]\n",
      " [  69.7   66.5   74.1   69.8   68.9   74.6   73.3   83.  1327. ]\n",
      " [  57.1   73.2   58.5   67.7   68.    75.3   61.4   65.  1360. ]\n",
      " [  67.2   76.2   71.6   70.9   69.5   71.7   72.8   81.1 1420. ]\n",
      " [  73.5   71.    72.2   68.5   73.    77.    69.    73.2 1322. ]\n",
      " [  58.1   67.7   67.3   66.8   63.8   72.9   63.5   80.4 1287. ]\n",
      " [  72.7   50.4   63.2   62.8   62.2   62.2   59.2   57.3 1285. ]\n",
      " [  56.4   78.1   69.8   69.6   62.6   74.1   66.1   71.9 1451. ]\n",
      " [  52.7   62.3   63.7   69.3   68.2   70.1   60.4   58.2 1326. ]\n",
      " [  68.9   79.2   75.6   74.6   70.8   76.1   77.1   76.  1580. ]\n",
      " [  67.3   71.8   69.3   76.8   67.3   69.7   66.1   75.8 1386. ]\n",
      " [  74.    85.1   82.    81.1   64.7   82.8   73.9   76.8 1640. ]\n",
      " [  68.7   79.5   73.6   79.4   72.6   77.    75.9   84.9 1404. ]\n",
      " [  59.7   78.5   58.3   63.5   65.9   68.2   69.    76.3 1313. ]\n",
      " [  71.1   77.6   63.5   73.7   77.4   80.5   70.4   74.1 1277. ]\n",
      " [  49.5   65.7   65.4   64.9   64.5   63.3   65.5   76.6 1224. ]\n",
      " [  56.4   67.4   67.5   60.5   62.8   64.1   59.9   77.  1290. ]\n",
      " [  50.    66.9   60.5   72.7   72.7   72.3   63.6   66.8 1211. ]\n",
      " [  59.9   81.    71.2   81.5   66.    82.4   67.6   66.1 1427. ]\n",
      " [  47.6   70.4   59.7   76.2   70.9   66.1   54.1   54.4 1335. ]\n",
      " [  67.9   79.    77.6   77.8   75.    81.6   71.4   81.7 1431. ]\n",
      " [  55.4   74.1   66.1   64.    66.    69.9   68.6   75.5 1314. ]\n",
      " [  72.3   64.8   70.    57.    64.    64.    60.2   64.7 1256. ]\n",
      " [  90.7   91.9   93.7   95.2   87.2   92.7   87.9   87.1 1981. ]\n",
      " [  67.    60.    78.8   74.4   65.3   75.3   72.1   72.3 1578. ]\n",
      " [  77.8   76.8   82.5   76.8   72.9   80.8   77.6   79.5 1530. ]\n",
      " [  50.    68.2   58.5   58.9   63.7   65.1   71.9   71.2 1184. ]\n",
      " [  62.8   70.6   74.3   70.2   70.6   75.6   64.5   64.2 1487. ]\n",
      " [  61.7   74.6   64.5   69.3   62.4   72.9   67.6   64.2 1326. ]\n",
      " [  56.5   74.4   67.    69.4   70.8   71.3   64.8   68.1 1285. ]\n",
      " [  44.5   69.9   66.3   65.2   64.1   65.4   68.8   65.6 1218. ]\n",
      " [  59.7   67.7   64.2   63.5   66.4   62.2   66.7   61.4 1214. ]\n",
      " [  63.    79.6   72.5   77.2   76.1   79.1   68.6   63.8 1397. ]\n",
      " [  69.1   80.3   75.5   79.1   75.1   78.9   71.4   78.2 1193. ]\n",
      " [  63.5   77.    67.9   68.8   69.8   70.4   69.4   64.4 1410. ]\n",
      " [  73.4   79.3   76.2   75.8   69.1   77.2   70.3   71.6 1485. ]\n",
      " [  58.8   70.3   62.9   67.5   66.4   66.8   60.8   56.7 1306. ]\n",
      " [  77.7   79.4   75.6   73.4   70.    80.6   70.7   75.3 1457. ]\n",
      " [  48.    63.8   56.8   60.1   57.9   62.2   55.    58.7 1248. ]\n",
      " [  43.7   69.5   49.8   58.4   62.7   64.3   55.1   47.2 1253. ]\n",
      " [  52.3   81.9   69.7   77.    71.3   79.1   58.4   41.4 1346. ]\n",
      " [  62.4   66.6   65.2   63.5   69.7   70.    69.4   67.1 1316. ]\n",
      " [  41.2   67.4   54.6   61.9   65.7   63.9   51.9   55.6 1265. ]\n",
      " [  64.5   87.4   75.2   81.6   61.7   84.2   67.3   74.3 1622. ]\n",
      " [  75.1   71.8   77.2   67.2   64.5   75.1   76.3   72.1 1274. ]\n",
      " [  67.    65.6   75.    63.4   72.6   77.1   69.3   79.8 1245. ]\n",
      " [  53.8   85.1   74.    79.5   72.5   81.5   77.2   71.9 1409. ]\n",
      " [  58.1   74.4   66.3   66.9   67.2   72.7   66.7   70.  1344. ]\n",
      " [  57.9   67.9   60.3   59.7   65.4   67.    62.8   67.6 1284. ]\n",
      " [  54.5   71.1   62.    63.1   66.5   67.6   64.4   64.  1355. ]\n",
      " [  61.5   78.7   66.7   71.6   66.6   73.4   63.7   66.2 1446. ]\n",
      " [  62.3   77.7   66.5   71.8   65.4   73.3   68.9   71.7 1473. ]\n",
      " [  52.8   69.8   66.7   60.7   63.5   64.5   61.7   61.1 1141. ]]\n",
      "(109, 9)\n"
     ]
    }
   ],
   "source": [
    "#  Also, save the values in df2 after dropping empty rows from df1\n",
    "# DEFINE Columns here -->\n",
    "# extract all the columns that we need\n",
    "df1=np.stack((df['Algebra2/Trigonometry'], df['English'], df['Geometry'],df['Global History and Geography'], df['Integrated Algebra'], df['Living Environment'], df['Physical Settings/Chemistry'], df['Physical Settings/Physics'], df['Average SAT Score (Total)'])).T\n",
    "\n",
    "#remove any empty rows\n",
    "df2=(df1[~np.isnan(df1).any(axis=1)])\n",
    "print(df2)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vector and fill with the output values (SAT Score). Create a matrix and fill with the features extracted from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109, 8)\n",
      "(109, 1)\n"
     ]
    }
   ],
   "source": [
    "# df3 will be our feature matrix\n",
    "df3 = df2[:,:8]\n",
    "#print(df3)\n",
    "\n",
    "# df4 will be the target vector\n",
    "df4 = df2[:,8:]\n",
    "#print(df4)\n",
    "X2 = np.array(df3)\n",
    "Y2= np.array(df4)\n",
    "\n",
    "X = np.array(df3)\n",
    "Y = np.array(df4)\n",
    "\n",
    "# Check the shape of x and y vectors.\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some reshaping here to convert the rank of the y vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109, 1)\n"
     ]
    }
   ],
   "source": [
    "Y=Y.reshape(Y.shape[0],1) \n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the value of n i.e. number of training examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    }
   ],
   "source": [
    "n=X.shape[0]\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending a Column of ones in x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.ones((Y.shape[0],1))\n",
    "X=np.hstack((a , X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109, 9)\n"
     ]
    }
   ],
   "source": [
    "# Shape of x should be [number of training examples (n)] x [number of features + 1]\n",
    "# the 1 comes from the column of ones we just added\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, w, n):\n",
    "    #Cost can be calculated using a single line of code.\n",
    "    # Remember w is a vector here.\n",
    "    cost=(1/(2*n)) * np.dot(np.ones(n).T, (np.dot(x,w)-y)**2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x , y , learning_rate , w , n , num_iters):\n",
    "    # write the updated value of w in temp \n",
    "    for i in range(num_iters):\n",
    "        # derivative vector is given by : X_train.Transpose *  (( X_train * w_vector)- y ) \n",
    "        temp =  w - (learning_rate/n)*np.dot(x.T,(np.dot(x,w)-y))\n",
    "        w = temp\n",
    "        cost= compute_cost(x , y , w , n)      \n",
    "            \n",
    "    return w  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating the Batch Gradient Descent Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrating the above function into a single function multiple_linear_reg_model_gda: This function uses gradient descent algorithm to minimize the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_linear_reg_model_gda(x , y , n , learning_rate , num_iters):\n",
    "    #initialize the values of parameter vector w. It should be a column vector of zeros of dimension(n,1)\n",
    "    w = np.zeros((x.shape[1],1))\n",
    "\n",
    "    #calculate the initial cost by calling the function you coded above.\n",
    "    initial_cost= compute_cost(x, y , w, n)\n",
    "    \n",
    "    #calculate the optimized value of gradients by calling the gradient_descent function coded above\n",
    "    w = gradient_descent(x , y , learning_rate , w , n , num_iters)\n",
    "    \n",
    "    #Calculate the cost with the optimized value of w by calling the cost function.\n",
    "    \n",
    "    final_cost = compute_cost(x, y , w, n)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call this function to find the optimized values of parameters w. First, set the values of learning_rate and num_iters. You may have to call this function several number of times with different values of num_iters and learning_rate to find the optimal values of w. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.77187846]\n",
      " [ 1.67041848]\n",
      " [ 2.22635535]\n",
      " [ 7.77416561]\n",
      " [ 0.79173384]\n",
      " [ 2.30669379]\n",
      " [-0.74975861]\n",
      " [ 1.4527537 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 0.0000029291\n",
    "num_iters = 10000000\n",
    "\n",
    "n = Y.shape[0]\n",
    "w = multiple_linear_reg_model_gda(X , Y , n , learning_rate , num_iters)\n",
    "print(w)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equation Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be used to cross-check the optimal values of w we just found using gradient descent above. These values should be same (or nearly same). First calculate q=inverse of (dot of (X.T,X)). Then w= dot of ( X.T , y) and then beta_vec= dot of (q,w). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.01291137]\n",
      " [ 1.81567226]\n",
      " [ 2.41995147]\n",
      " [ 8.45018001]\n",
      " [ 0.86058026]\n",
      " [ 2.50727586]\n",
      " [-0.81495501]\n",
      " [ 1.5790801 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "q = np.linalg.inv(np.dot(X2.T,X2))\n",
    "w = np.dot(X2.T,Y2)\n",
    "w_vec = np.dot(q,w)\n",
    "print(w_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Values of w should be approximately same as the ones from multiple_linear_reg_model_gda. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a predict function to plot results of learning algorithm and compare them with the plotted actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(x,w):\n",
    "    yhat = np.dot(x,w)\n",
    "    return yhat\n",
    "\n",
    "yhat = predict(X2,w)\n",
    "yhatNormalMethod = predict(X2,w_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#plot predicted values and y to see simiarities\n",
    "import matplotlib.pyplot as plt\n",
    "X2 = np.arange(0,Y2.shape[0])\n",
    "plt.scatter(X2,Y2,color ='red',s=8)\n",
    "#plt.scatter(y,yhat,color ='blue',s=8)\n",
    "plt.scatter(X2,yhatNormalMethod, color='blue',s=8)\n",
    "plt.ylabel(\"SAT Score\")\n",
    "plt.xlabel(\"\")\n",
    "plt.title(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8671015895873929\n",
      "0.6993012720703827\n"
     ]
    }
   ],
   "source": [
    "def calculateR2(y,yhat):\n",
    "    ym = np.mean(y)\n",
    "    rss = np.sum((y-yhat)**2)\n",
    "    ess = np.sum((yhat-ym)**2)\n",
    "    tss = np.sum((y-ym)**2)\n",
    "    return ess/tss #R^2\n",
    "\n",
    "rsq1 = calculateR2(Y2,yhat)\n",
    "rsq2 = calculateR2(Y2,yhatNormalMethod)\n",
    "print(rsq1) \n",
    "print(rsq2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1707.2436739828822 1002.8372987561428\n"
     ]
    }
   ],
   "source": [
    "#Min and max values for predicted set\n",
    "yhatmax = np.amax(yhat) \n",
    "yhatmin = np.amin(yhat) \n",
    "print(yhatmax,yhatmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
