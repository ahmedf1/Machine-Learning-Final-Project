{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import datasets, linear_model, preprocessing, model_selection\n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(df,norm_data):\n",
    "    df = df['Close'].values.reshape(-1,1)\n",
    "    norm_data = norm_data.reshape(-1,1)\n",
    "    scl = MinMaxScaler()\n",
    "    a = scl.fit_transform(df)\n",
    "    new = scl.inverse_transform(norm_data)\n",
    "\n",
    "def neural_net_model(X_data,input_dim):\n",
    "    W_1 = tf.Variable(tf.random_uniform([input_dim,10]))\n",
    "    b_1 = tf.Variable(tf.zeros([10]))\n",
    "    layer_1 = tf.add(tf.matmul(X_data,W_1), b_1)\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "    # layer 1 multiplying and adding bias then activation function\n",
    "    W_2 = tf.Variable(tf.random_uniform([10,10]))\n",
    "    b_2 = tf.Variable(tf.zeros([10]))\n",
    "    layer_2 = tf.add(tf.matmul(layer_1,W_2), b_2)\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # layer 2 multiplying and adding bias then activation function\n",
    "    W_O = tf.Variable(tf.random_uniform([10,1]))\n",
    "    b_O = tf.Variable(tf.zeros([1]))\n",
    "    output = tf.add(tf.matmul(layer_2,W_O), b_O)\n",
    "    \n",
    "    # O/p layer multiplying and adding bias then activation function\n",
    "    # notice output layer has one node only since performing #regression\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c_t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-69a7d8301ef7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mY_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;31m# Run cost and train with each sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mc_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mc_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch :'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Cost :'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'c_t' is not defined"
     ]
    }
   ],
   "source": [
    "#df = pd.read_csv('/Users/Farhad_Ahmed/Desktop/ML intros/ML Project/HS_Regents_Sat_Scores_2015.csv')\n",
    "df = pd.read_csv('HS_Regents_Sat_Scores_2015.csv')\n",
    "#print(df)\n",
    "#data = df.values\n",
    "\n",
    "# extract all the columns that we need\n",
    "df1=np.stack((df['Algebra2/Trigonometry'], df['English'], df['Geometry'],df['Global History and Geography'], df['Integrated Algebra'], df['Living Environment'], df['Physical Settings/Chemistry'], df['Physical Settings/Physics'], df['Average SAT Score (Total)'])).T\n",
    "\n",
    "#remove any empty rows\n",
    "df2=(df1[~np.isnan(df1).any(axis=1)])\n",
    "#print(df2)\n",
    "#print(df2.shape)\n",
    "\n",
    "# df3 will be our feature matrix\n",
    "df3 = df2[:,:8]\n",
    "#print(df3)\n",
    "\n",
    "# df4 will be the target vector\n",
    "df4 = df2[:,8:]\n",
    "#print(df4)\n",
    "\n",
    "X = np.array(df3)\n",
    "Y = np.array(df4)\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "for train, test in kf.split(X):\n",
    "    \n",
    "    X_tr, X_test, Y_tr, Y_test = X[train], X[test], Y[train], Y[test]\n",
    "     \n",
    "    scaler = MinMaxScaler() # For normalizing dataset\n",
    "\n",
    "    X_tr = scaler.fit_transform(X_tr)\n",
    "    Y_tr = scaler.fit_transform(Y_tr)\n",
    "    # y is output and x is features.\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    Y_test = scaler.fit_transform(Y_test)\n",
    "    \n",
    "    \"\"\"\n",
    "    neural_net_model is function applying 2 hidden layer feed forward neural net.\n",
    "    Weights and biases are abberviated as W_1,W_2 and b_1, b_2 \n",
    "    These are variables with will be updated during training.\n",
    "    \"\"\"\n",
    "    xs = tf.placeholder(\"float\")\n",
    "    ys = tf.placeholder(\"float\")\n",
    "    output = neural_net_model(xs,8)\n",
    "    cost = tf.reduce_mean(tf.square(output-ys))\n",
    "    # our mean squared error cost function\n",
    "    train = tf.train.GradientDescentOptimizer(0.001).minimize(cost)\n",
    "    # Gradinent Descent optimiztion just discussed above for updating weights and biases\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Initiate session and initialize all vaiables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for i in range(100):\n",
    "            for j in range(X_tr.shape[0]):\n",
    "                sess.run([cost,train],feed_dict = {xs:X_tr[j,:].reshape(1,8), ys:Y_tr[j]})\n",
    "                # Run cost and train with each sample\n",
    "            c_t.append(sess.run(cost, feed_dict={xs:X_tr,ys:y_tr}))\n",
    "            c_test.append(sess.run(cost, feed_dict={xs:X_test,ys:Y_test}))\n",
    "            print('Epoch :',i,'Cost :',c_t[i])\n",
    "        pred = sess.run(output, feed_dict={xs:X_test})\n",
    "        # predict output of test data after training\n",
    "        print('Cost :',sess.run(cost, feed_dict={xs:X_test,ys:Y_test}))\n",
    "        \n",
    "        \n",
    "        #numpy array to dataframe\n",
    "        index = ['Row'+str(i) for i in range(1, len(values)+1)]\n",
    "        df_test = pandas.DataFrame(Y_test, index=index)\n",
    "        \n",
    "        \n",
    "        Y_test = denormalize(df_test,Y_test)\n",
    "        pred = denormalize(df_test,pred)\n",
    "        #Denormalize data     \n",
    "        plt.plot(range(Y_test.shape[0]),Y_test,label=\"Original Data\")\n",
    "        plt.plot(range(Y_test.shape[0]),pred,label=\"Predicted Data\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.ylabel('SAT Score')\n",
    "        plt.xlabel('Regent Scores')\n",
    "        plt.title('Sat Prediction from Regents Scores')\n",
    "        plt.show()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
